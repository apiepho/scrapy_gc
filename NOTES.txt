
Since this is really a small tool, I am adding this shorter Notes file, rather than a
full blown README.txt and SETUP.txt.

Background
----------

This simple tool uses the Python based scrapy framework to crawl pages from GC and generate
a cache of html pages as files.  That cache can be used with gc_app. 

Installation
------------

Below are the simple command line steps for installing python and scrapy.

brew install python
pip install scrapy
mkdir scrapy_gc
cd scrapy_gc
scrapy startproject scrapy_gc scrapy_gc


Full documentation on scrapy and installation can be found at:
  https://doc.scrapy.org/en/latest/intro/overview.html

And the details on writing spiders is at:
https://doc.scrapy.org/en/latest/topics/spiders.html

This tool also relies on a headless browser called splash.  This is needed to navigate
and parse pages that are rendered with client-side javascript.


Will need docker for running splash.  I had problems installing and running docker from
brew.  Instead I was able to download and launch the docker tool from Mac Applications.

https://docs.docker.com/docker-for-mac/

then:
docker run -p 8050:8050 scrapinghub/splash


The following are needed to run splash from a scrapy spider:

brew install docker
pip install scrapy-splash
add the following to the settings.py file:
# example, use your own
  SPLASH_URL = 'http://192.168.59.103:8050'

  DOWNLOADER_MIDDLEWARES = {
      'scrapy_splash.SplashCookiesMiddleware': 723,
      'scrapy_splash.SplashMiddleware': 725,
      'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
  }
  
  SPIDER_MIDDLEWARES = {
	  'scrapy_splash.SplashDeduplicateArgsMiddleware': 100,
  }
 
  DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'

  HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'
 

The full instructions can be found at:
http://pydigger.com/pypi/scrapy-splash


Running
-------

First we need to have docker and splash running:

<Mac Applications::Docker...will place a whale icon on tool bar>
docker run -p 8050:8050 scrapinghub/splash

Then run the spider:
time scrapy crawl gc -a email="<login email>" -a password="<login password>" -a cache_dir="./cache_dir"


Details
-------

Most of the files here are auto generated by the startproject step.  All of the logic is in
...spiders/gc.py.