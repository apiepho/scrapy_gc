
Since this is really a small tool, I am adding this shorter Notes file, rather than a
full blown README.txt and SETUP.txt.

Background
----------

This simple tool uses the Python based scrapy framework to crawl pages from GC and generate
a cache of html pages as files.  That cache can be used with gc_app. 

FINISH
.
.
.
.
.
.
.
.
.


Installation
------------

Below are the simple command line steps for installing python and scrapy.

brew install python
pip install scrapy
pip install scrapy-splash
mkdir scrapy_gc
cd scrapy_gc
scrapy startproject scrapy_gc scrapy_gc


Full documentation on scrapy and installation can be found at:
  https://doc.scrapy.org/en/latest/intro/overview.html

And the details on writing spiders is at:
https://doc.scrapy.org/en/latest/topics/spiders.html


Running
-------

I typically use the following to list the categories:
  scrapy crawl craigslist -a url="https://fortcollins.craigslist.org" -a category=list --nolog

And this to get all the items (form category "Free") into a file "items.csv":
  rm items.csv
  scrapy crawl craigslist -o items.csv -a url="https://fortcollins.craigslist.org" -a category=zip --nolog
  tail items.csv

Alternatively you can give the whole url:
  rm items.csv
  scrapy crawl craigslist -o items.csv -a url="https://fortcollins.craigslist.org/serach/zip" --nolog
  tail items.csv

scrapy generates a lot of log messages by default, so I use --nologs to suppress.  scrapy also
appends to an output file, rather than overwrites it, so I remove it before each run.ÃŸ

Details
-------

Most of the files here are auto generated by the startproject step.  All of the logic is in
...spiders/craigslist.py.